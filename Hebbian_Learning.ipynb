{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y1MMzMvIefP",
        "outputId": "b7df4bf2-ccb8-407c-ee10-b304166d20a0"
      },
      "outputs": [],
      "source": [
        "#Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2 \\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hebbian Learning for OR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hebbian Learning for NOT Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, 1),  # NOT(-1) -> 1\n",
        "    (1, -1),  # NOT( 1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, bias = 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\ty\\tW1\")\n",
        "for x1, y in data:\n",
        "    print(f\"{x1}\\t{y}\\t{w1}\")\n",
        "    w1 += x1 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, bias={bias}\")\n",
        "print(f\"X1\\tW1\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, y in data:\n",
        "    net = w1 * x1 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{w1}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hebbian Learning for NOR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hebbian Learning for NAND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0D_FK1QJS2a",
        "outputId": "3ffb5522-fd94-42fc-acb3-7fa22d872654"
      },
      "outputs": [],
      "source": [
        "# Hebbian Learning for AND Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hebbian Learning for OR Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hebbian Learning for NOT Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, bias = 0, 0\n",
        "    print(f\"X1\\ty\\tW1  \")\n",
        "    for x1, y in data:\n",
        "        print(f\"{x1}\\t{y}\\t{w1}\")\n",
        "        w1 += x1 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, bias={bias}\")\n",
        "    return w1, bias\n",
        "\n",
        "def test_hebbian(data, w1, bias):\n",
        "    print(f\"X1\\tW1\\tb\\tY\")\n",
        "    for x1, y in data:\n",
        "        net = w1 * x1 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{w1}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1,  1),  # NOT(-1) -> 1\n",
        "    (1,  -1),  # NOT( 1) -> -1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hebbian Learning for NOR Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hebbian Learning for NAND Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehMRaeCBMW5c"
      },
      "outputs": [],
      "source": [
        "###Importing NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "##Creating Arrays\n",
        "\n",
        "a = np.array([1, 2, 3])  # 1D array\n",
        "b = np.array([[1, 2, 3], [4, 5, 6]])  # 2D array\n",
        "\n",
        "###Shape & Size\n",
        "\n",
        "print(a.shape)  # (3,)\n",
        "print(b.shape)  # (2, 3)\n",
        "print(b.size)   # 6 (total elements)\n",
        "\n",
        "####Generating Arrays\n",
        "\n",
        "zeros = np.zeros((2, 3))  # 2x3 matrix of zeros\n",
        "ones = np.ones((3, 3))    # 3x3 matrix of ones\n",
        "rand = np.random.rand(3, 3)  # 3x3 matrix of random values\n",
        "\n",
        "####Basic Operations\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "\n",
        "print(x + y)  # [5 7 9] (element-wise addition)\n",
        "print(x * y)  # [4 10 18] (element-wise multiplication)\n",
        "print(np.dot(x, y))  # 32 (dot product)\n",
        "\n",
        "###Matrix Multiplication\n",
        "\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "print(np.dot(A, B))  # Matrix multiplication\n",
        "\n",
        "####Mean, Sum, Max\n",
        "\n",
        "arr = np.array([1, 2, 3, 4])\n",
        "print(np.mean(arr))  # 2.5\n",
        "print(np.sum(arr))   # 10\n",
        "print(np.max(arr))   # 4\n",
        "\n",
        "###Conditional Selection\n",
        "\n",
        "arr = np.array([1, 2, 3, 4, 5])\n",
        "print(arr[arr > 2])  # [3 4 5] (elements that are greater than 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djVDwJVHNnjo",
        "outputId": "b2ce3423-5d04-459e-a789-9976030d4d70"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for OR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOT Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(1)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1,  1),  # NOT(-1) -> 1\n",
        "    (1,  -1),  # NOT(1) -> -1\n",
        "]\n",
        "\n",
        "print(f\"X1\\ty\\tW1\\tb\")\n",
        "for x1, y in data:\n",
        "    print(f\"{x1}\\t{y}\\t{w[0]}\\t{bias}\")\n",
        "    w += x1 * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tW1\\tb\\tY\")\n",
        "for x1, y in data:\n",
        "    net = np.dot(w, x1) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{w[0]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NAND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDWxKt4INJlX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1, -1],  # AND(-1, -1) -> -1\n",
        "    [-1,  1, -1],  # AND(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # AND( 1, -1) -> -1\n",
        "    [ 1,  1,  1]   # AND( 1,  1) -> 1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for OR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1, -1],  # OR(-1, -1) -> -1\n",
        "    [-1,  1, 1],   # OR(-1,  1) -> 1\n",
        "    [ 1, -1, 1],   # OR( 1, -1) -> 1\n",
        "    [ 1,  1,  1]   # OR( 1,  1) -> 1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOT Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(1)\n",
        "    bias = 0\n",
        "    print(f\"X1\\ty\\tW1\\tb\")\n",
        "    for x in data:\n",
        "        x1, y = x\n",
        "        print(f\"{(x1)}\\t{y}\\t{w[0]}\\t{bias}\")\n",
        "        w += x1 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tW1\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, y = x\n",
        "        net = np.dot(w, x1) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{w[0]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (np.array([-1]), 1),   # NOT(-1) -> 1\n",
        "    (np.array([1]), -1)    # NOT(1)  -> -1\n",
        "]\n",
        "\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1,  1],  # NOR(-1, -1) -> 1\n",
        "    [-1,  1, -1],  # NOR(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # NOR( 1, -1) -> -1\n",
        "    [ 1,  1, -1]   # NOR( 1,  1) -> -1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NAND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1,  1],  # NAND(-1, -1) -> 1\n",
        "    [-1,  1,  1],  # NAND(-1,  1) -> 1\n",
        "    [ 1, -1,  1],  # NAND( 1, -1) -> 1\n",
        "    [ 1,  1, -1]   # NAND( 1,  1) -> -1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuLrWNcCOrGH"
      },
      "outputs": [],
      "source": [
        "# Adaline Learning for AND Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    [-1, -1, -1],  # AND(-1, -1) -> -1\n",
        "    [-1,  1, -1],  # AND(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # AND( 1, -1) -> -1\n",
        "    [ 1,  1,  1]   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaline Learning for OR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaline Learning for NOT Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, 1),  # NOT(-1) -> 1\n",
        "    (1,  -1),  # NOT(1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\ty\\tW1\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{y}\\t{w[0]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tW1\\tb\\tY\")\n",
        "for x1, y in data:\n",
        "    net = w[0] * x1 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{w[0]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaline Learning for NOR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaline Learning for NAND Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaline Learning for XOR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # XOR(-1, -1) -> 1\n",
        "    (-1,  1,  -1),  # XOR(-1,  1) -> -1\n",
        "    ( 1, -1,  -1),  # XOR( 1, -1) -> -1\n",
        "    ( 1,  1, 1)   # XOR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaline Learning for XNOR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  -1),  # XNOR(-1, -1) -> -1\n",
        "    (-1,  1,  1),  # XNOR(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # XNOR( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # XNOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
