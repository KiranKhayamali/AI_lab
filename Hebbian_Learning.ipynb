{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
            "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
            "     ------------------------- ------------ 41.0/60.8 kB 326.8 kB/s eta 0:00:01\n",
            "     ------------------------------- ------ 51.2/60.8 kB 372.4 kB/s eta 0:00:01\n",
            "     -------------------------------------- 60.8/60.8 kB 358.9 kB/s eta 0:00:00\n",
            "Downloading numpy-2.2.5-cp312-cp312-win_amd64.whl (12.6 MB)\n",
            "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/12.6 MB 2.0 MB/s eta 0:00:07\n",
            "    --------------------------------------- 0.2/12.6 MB 2.5 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.5/12.6 MB 3.8 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 1.0/12.6 MB 6.4 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 2.0/12.6 MB 10.0 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 3.3/12.6 MB 12.3 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 4.1/12.6 MB 14.5 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 4.1/12.6 MB 14.5 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 7.4/12.6 MB 18.8 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 8.6/12.6 MB 20.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 10.0/12.6 MB 20.7 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.5/12.6 MB 31.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.6/12.6 MB 32.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.6/12.6 MB 28.5 MB/s eta 0:00:00\n",
            "Installing collected packages: numpy\n",
            "Successfully installed numpy-2.2.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y1MMzMvIefP",
        "outputId": "b7df4bf2-ccb8-407c-ee10-b304166d20a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2 \ty\tW1\tW2  \n",
            "-1\t-1\t-1\t0\t0\n",
            "-1\t1\t-1\t1\t1\n",
            "1\t-1\t-1\t2\t0\n",
            "1\t1\t1\t1\t1\n",
            "Trained Weights: w1=2, w2=2, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2\t2\t-6\t-1\n",
            "-1\t1\t2\t2\t-2\t-1\n",
            "1\t-1\t2\t2\t-2\t-1\n",
            "1\t1\t2\t2\t2\t1\n"
          ]
        }
      ],
      "source": [
        "#Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2 \\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t-1\t0\t0\n",
            "-1\t1\t1\t1\t1\n",
            "1\t-1\t1\t0\t2\n",
            "1\t1\t1\t1\t1\n",
            "Trained Weights: w1=2, w2=2, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2\t2\t-2\t-1\n",
            "-1\t1\t2\t2\t2\t1\n",
            "1\t-1\t2\t2\t2\t1\n",
            "1\t1\t2\t2\t6\t1\n"
          ]
        }
      ],
      "source": [
        "#Hebbian Learning for OR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\ty\tW1\n",
            "-1\t1\t0\n",
            "1\t-1\t-1\n",
            "Trained Weights: w1=-2, bias=0\n",
            "X1\tW1\tb\tY\n",
            "-1\t-2\t2\t1\n",
            "1\t-2\t-2\t-1\n"
          ]
        }
      ],
      "source": [
        "#Hebbian Learning for NOT Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, 1),  # NOT(-1) -> 1\n",
        "    (1, -1),  # NOT( 1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, bias = 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\ty\\tW1\")\n",
        "for x1, y in data:\n",
        "    print(f\"{x1}\\t{y}\\t{w1}\")\n",
        "    w1 += x1 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, bias={bias}\")\n",
        "print(f\"X1\\tW1\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, y in data:\n",
        "    net = w1 * x1 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{w1}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t1\t0\t0\n",
            "-1\t1\t-1\t-1\t-1\n",
            "1\t-1\t-1\t0\t-2\n",
            "1\t1\t-1\t-1\t-1\n",
            "Trained Weights: w1=-2, w2=-2, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2\t-2\t2\t1\n",
            "-1\t1\t-2\t-2\t-2\t-1\n",
            "1\t-1\t-2\t-2\t-2\t-1\n",
            "1\t1\t-2\t-2\t-6\t-1\n"
          ]
        }
      ],
      "source": [
        "#Hebbian Learning for NOR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t1\t0\t0\n",
            "-1\t1\t1\t-1\t-1\n",
            "1\t-1\t1\t-2\t0\n",
            "1\t1\t-1\t-1\t-1\n",
            "Trained Weights: w1=-2, w2=-2, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2\t-2\t6\t1\n",
            "-1\t1\t-2\t-2\t2\t1\n",
            "1\t-1\t-2\t-2\t2\t1\n",
            "1\t1\t-2\t-2\t-2\t-1\n"
          ]
        }
      ],
      "source": [
        "#Hebbian Learning for NAND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w1, w2, bias = 0, 0, 0\n",
        "\n",
        "# Training Phase\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "    w1 += x1 * y\n",
        "    w2 += x2 * y\n",
        "    bias += y\n",
        "\n",
        "print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "# Testing Phase\n",
        "for x1, x2, y in data:\n",
        "    net = w1 * x1 + w2 * x2 + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0D_FK1QJS2a",
        "outputId": "3ffb5522-fd94-42fc-acb3-7fa22d872654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t-1\t0\t0\n",
            "-1\t1\t-1\t1\t1\n",
            "1\t-1\t-1\t2\t0\n",
            "1\t1\t1\t1\t1\n",
            "Trained Weights: w1=2, w2=2, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2\t2\t-6\t-1\n",
            "-1\t1\t2\t2\t-2\t-1\n",
            "1\t-1\t2\t2\t-2\t-1\n",
            "1\t1\t2\t2\t2\t1\n"
          ]
        }
      ],
      "source": [
        "# Hebbian Learning for AND Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t-1\t0\t0\n",
            "-1\t1\t1\t1\t1\n",
            "1\t-1\t1\t0\t2\n",
            "1\t1\t1\t1\t1\n",
            "Trained Weights: w1=2, w2=2, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2\t2\t-2\t-1\n",
            "-1\t1\t2\t2\t2\t1\n",
            "1\t-1\t2\t2\t2\t1\n",
            "1\t1\t2\t2\t6\t1\n"
          ]
        }
      ],
      "source": [
        "# Hebbian Learning for OR Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\ty\tW1  \n",
            "-1\t1\t0\n",
            "1\t-1\t-1\n",
            "Trained Weights: w1=-2, bias=0\n",
            "X1\tW1\tb\tY\n",
            "-1\t-2\t2\t1\n",
            "1\t-2\t-2\t-1\n"
          ]
        }
      ],
      "source": [
        "# Hebbian Learning for NOT Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, bias = 0, 0\n",
        "    print(f\"X1\\ty\\tW1  \")\n",
        "    for x1, y in data:\n",
        "        print(f\"{x1}\\t{y}\\t{w1}\")\n",
        "        w1 += x1 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, bias={bias}\")\n",
        "    return w1, bias\n",
        "\n",
        "def test_hebbian(data, w1, bias):\n",
        "    print(f\"X1\\tW1\\tb\\tY\")\n",
        "    for x1, y in data:\n",
        "        net = w1 * x1 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{w1}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1,  1),  # NOT(-1) -> 1\n",
        "    (1,  -1),  # NOT( 1) -> -1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t1\t0\t0\n",
            "-1\t1\t-1\t-1\t-1\n",
            "1\t-1\t-1\t0\t-2\n",
            "1\t1\t-1\t-1\t-1\n",
            "Trained Weights: w1=-2, w2=-2, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2\t-2\t2\t1\n",
            "-1\t1\t-2\t-2\t-2\t-1\n",
            "1\t-1\t-2\t-2\t-2\t-1\n",
            "1\t1\t-2\t-2\t-6\t-1\n"
          ]
        }
      ],
      "source": [
        "# Hebbian Learning for NOR Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2  \n",
            "-1\t-1\t1\t0\t0\n",
            "-1\t1\t1\t-1\t-1\n",
            "1\t-1\t1\t-2\t0\n",
            "1\t1\t-1\t-1\t-1\n",
            "Trained Weights: w1=-2, w2=-2, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2\t-2\t6\t1\n",
            "-1\t1\t-2\t-2\t2\t1\n",
            "1\t-1\t-2\t-2\t2\t1\n",
            "1\t1\t-2\t-2\t-2\t-1\n"
          ]
        }
      ],
      "source": [
        "# Hebbian Learning for NAND Gate using function\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w1, w2, bias = 0, 0, 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2  \")\n",
        "    for x1, x2, y in data:\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w1}\\t{w2}\")\n",
        "        w1 += x1 * y\n",
        "        w2 += x2 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w1}, w2={w2}, bias={bias}\")\n",
        "    return w1, w2, bias\n",
        "\n",
        "def test_hebbian(data, w1, w2, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x1, x2, y in data:\n",
        "        net = w1 * x1 + w2 * x2 + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w1}\\t{w2}\\t{net}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "# Train the model\n",
        "w1, w2, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, w1, w2, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ehMRaeCBMW5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(2, 3)\n",
            "6\n",
            "[5 7 9]\n",
            "[ 4 10 18]\n",
            "32\n",
            "[[19 22]\n",
            " [43 50]]\n",
            "2.5\n",
            "10\n",
            "4\n",
            "[3 4 5]\n"
          ]
        }
      ],
      "source": [
        "###Importing NumPy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "##Creating Arrays\n",
        "\n",
        "a = np.array([1, 2, 3])  # 1D array\n",
        "b = np.array([[1, 2, 3], [4, 5, 6]])  # 2D array\n",
        "\n",
        "###Shape & Size\n",
        "\n",
        "print(a.shape)  # (3,)\n",
        "print(b.shape)  # (2, 3)\n",
        "print(b.size)   # 6 (total elements)\n",
        "\n",
        "####Generating Arrays\n",
        "\n",
        "zeros = np.zeros((2, 3))  # 2x3 matrix of zeros\n",
        "ones = np.ones((3, 3))    # 3x3 matrix of ones\n",
        "rand = np.random.rand(3, 3)  # 3x3 matrix of random values\n",
        "\n",
        "####Basic Operations\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([4, 5, 6])\n",
        "\n",
        "print(x + y)  # [5 7 9] (element-wise addition)\n",
        "print(x * y)  # [4 10 18] (element-wise multiplication)\n",
        "print(np.dot(x, y))  # 32 (dot product)\n",
        "\n",
        "###Matrix Multiplication\n",
        "\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "\n",
        "print(np.dot(A, B))  # Matrix multiplication\n",
        "\n",
        "####Mean, Sum, Max\n",
        "\n",
        "arr = np.array([1, 2, 3, 4])\n",
        "print(np.mean(arr))  # 2.5\n",
        "print(np.sum(arr))   # 10\n",
        "print(np.max(arr))   # 4\n",
        "\n",
        "###Conditional Selection\n",
        "\n",
        "arr = np.array([1, 2, 3, 4, 5])\n",
        "print(arr[arr > 2])  # [3 4 5] (elements that are greater than 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djVDwJVHNnjo",
        "outputId": "b2ce3423-5d04-459e-a789-9976030d4d70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t-1\t0.0\t0.0\t0\n",
            "-1\t1\t-1\t1.0\t1.0\t-1\n",
            "1\t-1\t-1\t2.0\t0.0\t-2\n",
            "1\t1\t1\t1.0\t1.0\t-3\n",
            "Trained Weights: w1=2.0, w2=2.0, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2.0\t2.0\t-2\t-1\n",
            "-1\t1\t2.0\t2.0\t-2\t-1\n",
            "1\t-1\t2.0\t2.0\t-2\t-1\n",
            "1\t1\t2.0\t2.0\t-2\t1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # AND(-1, -1) -> -1\n",
        "    (-1,  1, -1),  # AND(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # AND( 1, -1) -> -1\n",
        "    ( 1,  1,  1)   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t-1\t0.0\t0.0\t0\n",
            "-1\t1\t1\t1.0\t1.0\t-1\n",
            "1\t-1\t1\t0.0\t2.0\t0\n",
            "1\t1\t1\t1.0\t1.0\t1\n",
            "Trained Weights: w1=2.0, w2=2.0, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2.0\t2.0\t2\t-1\n",
            "-1\t1\t2.0\t2.0\t2\t1\n",
            "1\t-1\t2.0\t2.0\t2\t1\n",
            "1\t1\t2.0\t2.0\t2\t1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for OR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\ty\tW1\tb\n",
            "-1\t1\t0.0\t0\n",
            "1\t-1\t-1.0\t1\n",
            "Trained Weights: w1=-2.0, bias=0\n",
            "X1\tW1\tb\tY\n",
            "-1\t-2.0\t0\t1\n",
            "1\t-2.0\t0\t-1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOT Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(1)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1,  1),  # NOT(-1) -> 1\n",
        "    (1,  -1),  # NOT(1) -> -1\n",
        "]\n",
        "\n",
        "print(f\"X1\\ty\\tW1\\tb\")\n",
        "for x1, y in data:\n",
        "    print(f\"{x1}\\t{y}\\t{w[0]}\\t{bias}\")\n",
        "    w += x1 * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tW1\\tb\\tY\")\n",
        "for x1, y in data:\n",
        "    net = np.dot(w, x1) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{w[0]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t1\t0.0\t0.0\t0\n",
            "-1\t1\t-1\t-1.0\t-1.0\t1\n",
            "1\t-1\t-1\t0.0\t-2.0\t0\n",
            "1\t1\t-1\t-1.0\t-1.0\t-1\n",
            "Trained Weights: w1=-2.0, w2=-2.0, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2.0\t-2.0\t-2\t1\n",
            "-1\t1\t-2.0\t-2.0\t-2\t-1\n",
            "1\t-1\t-2.0\t-2.0\t-2\t-1\n",
            "1\t1\t-2.0\t-2.0\t-2\t-1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t1\t0.0\t0.0\t0\n",
            "-1\t1\t1\t-1.0\t-1.0\t1\n",
            "1\t-1\t1\t-2.0\t0.0\t2\n",
            "1\t1\t-1\t-1.0\t-1.0\t3\n",
            "Trained Weights: w1=-2.0, w2=-2.0, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2.0\t-2.0\t2\t1\n",
            "-1\t1\t-2.0\t-2.0\t2\t1\n",
            "1\t-1\t-2.0\t-2.0\t2\t1\n",
            "1\t1\t-2.0\t-2.0\t2\t-1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NAND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "w = np.zeros(2)\n",
        "bias = 0\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "for x1, x2, y in data:\n",
        "    print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "    w += np.array([x1, x2]) * y\n",
        "    bias += y\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = np.dot(w, np.array([x1, x2])) + bias\n",
        "    r = 1 if net >= 0 else -1\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zDWxKt4INJlX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t-1\t0.0\t0.0\t0\n",
            "-1\t1\t-1\t1.0\t1.0\t-1\n",
            "1\t-1\t-1\t2.0\t0.0\t-2\n",
            "1\t1\t1\t1.0\t1.0\t-3\n",
            "Trained Weights: w1=2.0, w2=2.0, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2.0\t2.0\t-2\t-1\n",
            "-1\t1\t2.0\t2.0\t-2\t-1\n",
            "1\t-1\t2.0\t2.0\t-2\t-1\n",
            "1\t1\t2.0\t2.0\t-2\t1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for AND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1, -1],  # AND(-1, -1) -> -1\n",
        "    [-1,  1, -1],  # AND(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # AND( 1, -1) -> -1\n",
        "    [ 1,  1,  1]   # AND( 1,  1) -> 1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t-1\t0.0\t0.0\t0\n",
            "-1\t1\t1\t1.0\t1.0\t-1\n",
            "1\t-1\t1\t0.0\t2.0\t0\n",
            "1\t1\t1\t1.0\t1.0\t1\n",
            "Trained Weights: w1=2.0, w2=2.0, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t2.0\t2.0\t2\t-1\n",
            "-1\t1\t2.0\t2.0\t2\t1\n",
            "1\t-1\t2.0\t2.0\t2\t1\n",
            "1\t1\t2.0\t2.0\t2\t1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for OR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1, -1],  # OR(-1, -1) -> -1\n",
        "    [-1,  1, 1],   # OR(-1,  1) -> 1\n",
        "    [ 1, -1, 1],   # OR( 1, -1) -> 1\n",
        "    [ 1,  1,  1]   # OR( 1,  1) -> 1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\ty\tW1\tb\n",
            "[-1]\t1\t0.0\t0\n",
            "[1]\t-1\t-1.0\t1\n",
            "Trained Weights: w1=-2.0, bias=0\n",
            "X1\tW1\tb\tY\n",
            "[-1]\t-2.0\t0\t1\n",
            "[1]\t-2.0\t0\t-1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOT Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(1)\n",
        "    bias = 0\n",
        "    print(f\"X1\\ty\\tW1\\tb\")\n",
        "    for x in data:\n",
        "        x1, y = x\n",
        "        print(f\"{(x1)}\\t{y}\\t{w[0]}\\t{bias}\")\n",
        "        w += x1 * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tW1\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, y = x\n",
        "        net = np.dot(w, x1) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{w[0]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (np.array([-1]), 1),   # NOT(-1) -> 1\n",
        "    (np.array([1]), -1)    # NOT(1)  -> -1\n",
        "]\n",
        "\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t1\t0.0\t0.0\t0\n",
            "-1\t1\t-1\t-1.0\t-1.0\t1\n",
            "1\t-1\t-1\t0.0\t-2.0\t0\n",
            "1\t1\t-1\t-1.0\t-1.0\t-1\n",
            "Trained Weights: w1=-2.0, w2=-2.0, bias=-2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2.0\t-2.0\t-2\t1\n",
            "-1\t1\t-2.0\t-2.0\t-2\t-1\n",
            "1\t-1\t-2.0\t-2.0\t-2\t-1\n",
            "1\t1\t-2.0\t-2.0\t-2\t-1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NOR Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1,  1],  # NOR(-1, -1) -> 1\n",
        "    [-1,  1, -1],  # NOR(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # NOR( 1, -1) -> -1\n",
        "    [ 1,  1, -1]   # NOR( 1,  1) -> -1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\n",
            "-1\t-1\t1\t0.0\t0.0\t0\n",
            "-1\t1\t1\t-1.0\t-1.0\t1\n",
            "1\t-1\t1\t-2.0\t0.0\t2\n",
            "1\t1\t-1\t-1.0\t-1.0\t3\n",
            "Trained Weights: w1=-2.0, w2=-2.0, bias=2\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-2.0\t-2.0\t2\t1\n",
            "-1\t1\t-2.0\t-2.0\t2\t1\n",
            "1\t-1\t-2.0\t-2.0\t2\t1\n",
            "1\t1\t-2.0\t-2.0\t2\t-1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Hebbian Learning for NAND Gate\n",
        "# Using -1 instead of 0 and threshold of 0\n",
        "\n",
        "def train_hebbian(data):\n",
        "    w = np.zeros(2)\n",
        "    bias = 0\n",
        "    print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\")\n",
        "        w += np.array([x1, x2]) * y\n",
        "        bias += y\n",
        "    print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "    return w, bias\n",
        "\n",
        "def test_hebbian(data, w, bias):\n",
        "    print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "    for x in data:\n",
        "        x1, x2, y = x\n",
        "        net = np.dot(w, np.array([x1, x2])) + bias\n",
        "        r = 1 if net >= 0 else -1\n",
        "        print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{r}\")\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = np.array([\n",
        "    [-1, -1,  1],  # NAND(-1, -1) -> 1\n",
        "    [-1,  1,  1],  # NAND(-1,  1) -> 1\n",
        "    [ 1, -1,  1],  # NAND( 1, -1) -> 1\n",
        "    [ 1,  1, -1]   # NAND( 1,  1) -> -1\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_hebbian(data)\n",
        "\n",
        "# Test the model\n",
        "test_hebbian(data, weights, bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QuLrWNcCOrGH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\tY\n",
            "-1\t-1\t-1\t0.1\t0.1\t-0.1\t0.0\n",
            "-1\t1\t-1\t0.19\t0.009999999999999995\t-0.19\t-0.1\n",
            "1\t-1\t-1\t0.091\t0.109\t-0.28900000000000003\t-0.010000000000000009\n",
            "1\t1\t1\t0.1999\t0.21789999999999998\t-0.18010000000000004\t-0.08900000000000002\n",
            "-1\t-1\t-1\t0.24011\t0.25811\t-0.22031000000000003\t-0.5979\n",
            "-1\t1\t-1\t0.319879\t0.178341\t-0.30007900000000004\t-0.20231000000000002\n",
            "1\t-1\t-1\t0.23573310000000003\t0.26248689999999997\t-0.38422490000000004\t-0.15854100000000002\n",
            "1\t1\t1\t0.32433359000000006\t0.35108739\t-0.29562441\t0.11399509999999996\n",
            "-1\t-1\t-1\t0.32722905100000005\t0.353982851\t-0.298519871\t-0.97104539\n",
            "-1\t1\t-1\t0.40005244390000005\t0.2811594581\t-0.3713432639\t-0.27176607100000005\n",
            "1\t-1\t-1\t0.32529747171000006\t0.35591443029\t-0.44609823609\t-0.2524502780999999\n",
            "1\t1\t1\t0.40178610511900004\t0.43240306369899995\t-0.36960960268099996\t0.23511366591000005\n",
            "-1\t-1\t-1\t0.38140622796910006\t0.4120231865491\t-0.3492297255311\t-1.203798771499\n",
            "-1\t1\t-1\t0.4495449512739901\t0.34388446324420996\t-0.41736844883599\t-0.3186127669511001\n",
            "1\t-1\t-1\t0.38071574735461106\t0.412713667163589\t-0.486197652755369\t-0.3117079608062099\n",
            "1\t1\t1\t0.44999257117832797\t0.4819904909873059\t-0.4169208289316521\t0.307231761762831\n",
            "-1\t-1\t-1\t0.4151021820685994\t0.4471001018775773\t-0.38203043982192353\t-1.348903891097286\n",
            "-1\t1\t-1\t0.48009893006730486\t0.38210335387887184\t-0.447027187820629\t-0.3500325200129456\n",
            "1\t-1\t-1\t0.41500209123052445\t0.44720019271565226\t-0.5121240266574094\t-0.349031611632196\n",
            "1\t1\t1\t0.4799942655016477\t0.5121923669867755\t-0.44713185238628617\t0.3500782572887673\n",
            "-1\t-1\t-1\t0.43606241701417675\t0.46826051849930456\t-0.4032000038988152\t-1.4393184848747094\n",
            "-1\t1\t-1\t0.498962226772808\t0.4053607087406733\t-0.46609981365744646\t-0.3710019024136874\n",
            "1\t-1\t-1\t0.43621205633533916\t0.46811087917814215\t-0.5288499840949152\t-0.3724982956253118\n",
            "1\t1\t1\t0.4986647611934826\t0.5305635840362856\t-0.4663972792367718\t0.37547295141856607\n",
            "-1\t-1\t-1\t0.44910219874682855\t0.48100102158963154\t-0.4168347167901178\t-1.49562562446654\n",
            "-1\t1\t-1\t0.5106086093520971\t0.419494610984363\t-0.4783411273953863\t-0.3849358939473148\n",
            "1\t-1\t-1\t0.4493313222548623\t0.4807718980815978\t-0.539618414492621\t-0.3872271290276523\n",
            "1\t1\t1\t0.5102828416704783\t0.5417234174972139\t-0.47866689507700494\t0.39048480584383904\n",
            "-1\t-1\t-1\t0.4572155262460086\t0.4886561020727441\t-0.4255995796525352\t-1.5306731542446972\n",
            "-1\t1\t-1\t0.5177996258634286\t0.42807200245532406\t-0.48618367926995526\t-0.39415900382579966\n",
            "1\t-1\t-1\t0.4574452314496137\t0.488426396869139\t-0.5465380736837702\t-0.3964560558618507\n",
            "1\t1\t1\t0.5175118759861155\t0.5484930414056408\t-0.48647142914726843\t0.39933355463498255\n",
            "-1\t-1\t-1\t0.462264241332213\t0.49324540675173834\t-0.43122379449336595\t-1.5524763465390248\n",
            "-1\t1\t-1\t0.5222399784248289\t0.4332696696591224\t-0.4911995315859819\t-0.4002426290738406\n",
            "1\t-1\t-1\t0.4624629007068565\t0.49304674737709486\t-0.5509766093039543\t-0.40222922282027535\n",
            "1\t1\t1\t0.5220095968288567\t0.5525934434990951\t-0.491429913181954\t0.404533038779997\n",
            "-1\t-1\t-1\t0.46540630147786616\t0.49599014814810455\t-0.43482661783096344\t-1.5660329535099058\n",
            "-1\t1\t-1\t0.5249820243617936\t0.43641442526417706\t-0.4944023407148909\t-0.40424277116072505\n",
            "1\t-1\t-1\t0.4655654985235211\t0.4958309511024496\t-0.5538188665531635\t-0.40583474161727434\n",
            "1\t1\t1\t0.5248077402162403\t0.5550731927951689\t-0.4945766248604442\t0.4075775830728072\n",
            "Trained Weights: w1=0.5248077402162403, w2=0.5550731927951689, bias=-0.4945766248604442\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t0.5248077402162403\t0.5550731927951689\t-0.4945766248604442\t-1.5744575578718534\n",
            "-1\t1\t0.5248077402162403\t0.5550731927951689\t-0.4945766248604442\t-0.46431117228151564\n",
            "1\t-1\t0.5248077402162403\t0.5550731927951689\t-0.4945766248604442\t-0.5248420774393727\n",
            "1\t1\t0.5248077402162403\t0.5550731927951689\t-0.4945766248604442\t0.585304308150965\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for AND Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    [-1, -1, -1],  # AND(-1, -1) -> -1\n",
        "    [-1,  1, -1],  # AND(-1,  1) -> -1\n",
        "    [ 1, -1, -1],  # AND( 1, -1) -> -1\n",
        "    [ 1,  1,  1]   # AND( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\tY\n",
            "-1\t-1\t-1\t0.1\t0.1\t-0.1\t0.0\n",
            "-1\t1\t1\t-0.010000000000000009\t0.21000000000000002\t0.010000000000000009\t-0.1\n",
            "1\t-1\t1\t0.11099999999999999\t0.08900000000000002\t0.131\t-0.21000000000000002\n",
            "1\t1\t1\t0.1779\t0.15590000000000004\t0.19790000000000002\t0.331\n",
            "-1\t-1\t-1\t0.26431\t0.24231000000000003\t0.11149000000000002\t-0.13590000000000002\n",
            "-1\t1\t1\t0.173259\t0.333361\t0.20254100000000003\t0.08949000000000006\n",
            "1\t-1\t1\t0.2690151\t0.2376049\t0.29829710000000004\t0.042439000000000004\n",
            "1\t1\t1\t0.28852339\t0.25711319\t0.31780539\t0.8049171000000002\n",
            "-1\t-1\t-1\t0.365740271\t0.334330071\t0.240588509\t-0.22783119000000002\n",
            "-1\t1\t1\t0.2866581019\t0.4134122401\t0.3196706781\t0.209178309\n",
            "1\t-1\t1\t0.36736644791\t0.33270389409\t0.40037902411\t0.19291653990000002\n",
            "1\t1\t1\t0.35732151129899997\t0.322658957479\t0.39033408749899995\t1.10044936611\n",
            "-1\t-1\t-1\t0.4283568731711\t0.3936943193511\t0.3192987256268999\t-0.28964638127900005\n",
            "-1\t1\t1\t0.35682049035179\t0.46523070217041\t0.39083510844620994\t0.28463617180689993\n",
            "1\t-1\t1\t0.42857800068903096\t0.393473191833169\t0.4625926187834509\t0.2824248966275899\n",
            "1\t1\t1\t0.40011361955846586\t0.3650088107026039\t0.43412823765288583\t1.2846438113056509\n",
            "-1\t-1\t-1\t0.4670142002976475\t0.43190939144178553\t0.3672276569137042\t-0.3309941926081839\n",
            "-1\t1\t1\t0.40022648510343173\t0.4986971066360013\t0.43401537210791996\t0.3321228480578422\n",
            "1\t-1\t1\t0.4666720100458967\t0.43225158169353634\t0.5004608970503849\t0.3355447505753504\n",
            "1\t1\t1\t0.4267335611669149\t0.39231313281455454\t0.4605224481714031\t1.399384488789818\n",
            "-1\t-1\t-1\t0.49088113658590826\t0.4564607082335479\t0.39637487275240973\t-0.35852424581006637\n",
            "-1\t1\t1\t0.42707658102591317\t0.520265263793543\t0.4601794283124048\t0.36195444440004937\n",
            "1\t-1\t1\t0.4903775064714357\t0.45696433834802047\t0.5234803537579273\t0.366990745544775\n",
            "1\t1\t1\t0.4432952866136973\t0.4098821184902821\t0.476398133900189\t1.4708221985773835\n",
            "-1\t-1\t-1\t0.5056173594933183\t0.47220419136990305\t0.41407606102056804\t-0.37677927120379046\n",
            "-1\t1\t1\t0.44368364878303357\t0.5341379020801877\t0.4760097717308528\t0.38066289289715277\n",
            "1\t-1\t1\t0.5051280969396637\t0.4726934539235576\t0.537454219887483\t0.3855555184336986\n",
            "1\t1\t1\t0.4536005198645932\t0.42116587684848716\t0.48592664281241255\t1.5152757707507043\n",
            "-1\t-1\t-1\t0.5147165444745264\t0.48228190145842037\t0.42481061820247934\t-0.3888397539006678\n",
            "-1\t1\t1\t0.45395414199316375\t0.543044303939783\t0.485573020683842\t0.3923759751863733\n",
            "1\t-1\t1\t0.5143058561194415\t0.48269258981350527\t0.5459247348101197\t0.3964828587372228\n",
            "1\t1\t1\t0.46001353804513484\t0.42840027173919865\t0.4916324167358131\t1.5429231807430663\n",
            "-1\t-1\t-1\t0.5203353987402828\t0.4887221324343466\t0.43131055604066515\t-0.3967813930485204\n",
            "-1\t1\t1\t0.46030512771375565\t0.5487524034608737\t0.4913408270671923\t0.39969728973472896\n",
            "1\t-1\t1\t0.5200157725817482\t0.4890417585928811\t0.5510514719351849\t0.4028935513200742\n",
            "1\t1\t1\t0.4640048722707668\t0.4330308582818997\t0.49504057162420345\t1.5601090031098144\n",
            "-1\t-1\t-1\t0.5238053563779205\t0.4928313423890534\t0.43524008751704973\t-0.40199515892846305\n",
            "-1\t1\t1\t0.4642319637307387\t0.5524047350362351\t0.4948134801642315\t0.40426607352818267\n",
            "1\t-1\t1\t0.5235678928448652\t0.4930688059221086\t0.554149409278358\t0.4066407088587351\n",
            "1\t1\t1\t0.46648928204033197\t0.4359901951175754\t0.4970707984738248\t1.5707861080453318\n",
            "Trained Weights: w1=0.46648928204033197, w2=0.4359901951175754, bias=0.4970707984738248\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t0.46648928204033197\t0.4359901951175754\t0.4970707984738248\t-0.40540867868408265\n",
            "-1\t1\t0.46648928204033197\t0.4359901951175754\t0.4970707984738248\t0.46657171155106825\n",
            "1\t-1\t0.46648928204033197\t0.4359901951175754\t0.4970707984738248\t0.5275698853965813\n",
            "1\t1\t0.46648928204033197\t0.4359901951175754\t0.4970707984738248\t1.3995502756317322\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for OR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1, -1),  # OR(-1, -1) -> -1\n",
        "    (-1,  1, 1),   # OR(-1,  1) -> 1\n",
        "    ( 1, -1, 1),   # OR( 1, -1) -> 1\n",
        "    ( 1,  1,  1)   # OR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\ty\tW1\tb\tY\n",
            "-1\t1\t-0.1\t0.1\t0.0\n",
            "1\t-1\t-0.2\t0.0\t0.0\n",
            "-1\t1\t-0.28\t0.08000000000000002\t0.2\n",
            "1\t-1\t-0.36000000000000004\t0.0\t-0.2\n",
            "-1\t1\t-0.42400000000000004\t0.06399999999999999\t0.36000000000000004\n",
            "1\t-1\t-0.48800000000000004\t0.0\t-0.36000000000000004\n",
            "-1\t1\t-0.5392\t0.0512\t0.48800000000000004\n",
            "1\t-1\t-0.5904\t0.0\t-0.488\n",
            "-1\t1\t-0.63136\t0.040959999999999996\t0.5904\n",
            "1\t-1\t-0.67232\t0.0\t-0.5904\n",
            "-1\t1\t-0.705088\t0.032768\t0.67232\n",
            "1\t-1\t-0.7378560000000001\t0.0\t-0.67232\n",
            "-1\t1\t-0.7640704\t0.026214399999999995\t0.7378560000000001\n",
            "1\t-1\t-0.7902848\t0.0\t-0.7378560000000001\n",
            "-1\t1\t-0.81125632\t0.02097152\t0.7902848\n",
            "1\t-1\t-0.83222784\t0.0\t-0.7902848\n",
            "-1\t1\t-0.8490050560000001\t0.016777215999999994\t0.83222784\n",
            "1\t-1\t-0.8657822720000001\t0.0\t-0.83222784\n",
            "-1\t1\t-0.8792040448000001\t0.013421772799999988\t0.8657822720000001\n",
            "1\t-1\t-0.8926258176000001\t0.0\t-0.8657822720000001\n",
            "Trained Weights: w1=-0.8926258176000001, bias=0.0\n",
            "X1\tW1\tb\tY\n",
            "-1\t-0.8926258176000001\t0.0\t0.8926258176000001\n",
            "1\t-0.8926258176000001\t0.0\t-0.8926258176000001\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for NOT Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, 1),  # NOT(-1) -> 1\n",
        "    (1,  -1),  # NOT(1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\ty\\tW1\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{y}\\t{w[0]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tW1\\tb\\tY\")\n",
        "for x1, y in data:\n",
        "    net = w[0] * x1 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{w[0]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\tY\n",
            "-1\t-1\t1\t-0.1\t-0.1\t0.1\t0.0\n",
            "-1\t1\t-1\t0.010000000000000009\t-0.21000000000000002\t-0.010000000000000009\t0.1\n",
            "1\t-1\t-1\t-0.11099999999999999\t-0.08900000000000002\t-0.131\t0.21000000000000002\n",
            "1\t1\t-1\t-0.1779\t-0.15590000000000004\t-0.19790000000000002\t-0.331\n",
            "-1\t-1\t1\t-0.26431\t-0.24231000000000003\t-0.11149000000000002\t0.13590000000000002\n",
            "-1\t1\t-1\t-0.173259\t-0.333361\t-0.20254100000000003\t-0.08949000000000006\n",
            "1\t-1\t-1\t-0.2690151\t-0.2376049\t-0.29829710000000004\t-0.042439000000000004\n",
            "1\t1\t-1\t-0.28852339\t-0.25711319\t-0.31780539\t-0.8049171000000002\n",
            "-1\t-1\t1\t-0.365740271\t-0.334330071\t-0.240588509\t0.22783119000000002\n",
            "-1\t1\t-1\t-0.2866581019\t-0.4134122401\t-0.3196706781\t-0.209178309\n",
            "1\t-1\t-1\t-0.36736644791\t-0.33270389409\t-0.40037902411\t-0.19291653990000002\n",
            "1\t1\t-1\t-0.35732151129899997\t-0.322658957479\t-0.39033408749899995\t-1.10044936611\n",
            "-1\t-1\t1\t-0.4283568731711\t-0.3936943193511\t-0.3192987256268999\t0.28964638127900005\n",
            "-1\t1\t-1\t-0.35682049035179\t-0.46523070217041\t-0.39083510844620994\t-0.28463617180689993\n",
            "1\t-1\t-1\t-0.42857800068903096\t-0.393473191833169\t-0.4625926187834509\t-0.2824248966275899\n",
            "1\t1\t-1\t-0.40011361955846586\t-0.3650088107026039\t-0.43412823765288583\t-1.2846438113056509\n",
            "-1\t-1\t1\t-0.4670142002976475\t-0.43190939144178553\t-0.3672276569137042\t0.3309941926081839\n",
            "-1\t1\t-1\t-0.40022648510343173\t-0.4986971066360013\t-0.43401537210791996\t-0.3321228480578422\n",
            "1\t-1\t-1\t-0.4666720100458967\t-0.43225158169353634\t-0.5004608970503849\t-0.3355447505753504\n",
            "1\t1\t-1\t-0.4267335611669149\t-0.39231313281455454\t-0.4605224481714031\t-1.399384488789818\n",
            "-1\t-1\t1\t-0.49088113658590826\t-0.4564607082335479\t-0.39637487275240973\t0.35852424581006637\n",
            "-1\t1\t-1\t-0.42707658102591317\t-0.520265263793543\t-0.4601794283124048\t-0.36195444440004937\n",
            "1\t-1\t-1\t-0.4903775064714357\t-0.45696433834802047\t-0.5234803537579273\t-0.366990745544775\n",
            "1\t1\t-1\t-0.4432952866136973\t-0.4098821184902821\t-0.476398133900189\t-1.4708221985773835\n",
            "-1\t-1\t1\t-0.5056173594933183\t-0.47220419136990305\t-0.41407606102056804\t0.37677927120379046\n",
            "-1\t1\t-1\t-0.44368364878303357\t-0.5341379020801877\t-0.4760097717308528\t-0.38066289289715277\n",
            "1\t-1\t-1\t-0.5051280969396637\t-0.4726934539235576\t-0.537454219887483\t-0.3855555184336986\n",
            "1\t1\t-1\t-0.4536005198645932\t-0.42116587684848716\t-0.48592664281241255\t-1.5152757707507043\n",
            "-1\t-1\t1\t-0.5147165444745264\t-0.48228190145842037\t-0.42481061820247934\t0.3888397539006678\n",
            "-1\t1\t-1\t-0.45395414199316375\t-0.543044303939783\t-0.485573020683842\t-0.3923759751863733\n",
            "1\t-1\t-1\t-0.5143058561194415\t-0.48269258981350527\t-0.5459247348101197\t-0.3964828587372228\n",
            "1\t1\t-1\t-0.46001353804513484\t-0.42840027173919865\t-0.4916324167358131\t-1.5429231807430663\n",
            "-1\t-1\t1\t-0.5203353987402828\t-0.4887221324343466\t-0.43131055604066515\t0.3967813930485204\n",
            "-1\t1\t-1\t-0.46030512771375565\t-0.5487524034608737\t-0.4913408270671923\t-0.39969728973472896\n",
            "1\t-1\t-1\t-0.5200157725817482\t-0.4890417585928811\t-0.5510514719351849\t-0.4028935513200742\n",
            "1\t1\t-1\t-0.4640048722707668\t-0.4330308582818997\t-0.49504057162420345\t-1.5601090031098144\n",
            "-1\t-1\t1\t-0.5238053563779205\t-0.4928313423890534\t-0.43524008751704973\t0.40199515892846305\n",
            "-1\t1\t-1\t-0.4642319637307387\t-0.5524047350362351\t-0.4948134801642315\t-0.40426607352818267\n",
            "1\t-1\t-1\t-0.5235678928448652\t-0.4930688059221086\t-0.554149409278358\t-0.4066407088587351\n",
            "1\t1\t-1\t-0.46648928204033197\t-0.4359901951175754\t-0.4970707984738248\t-1.5707861080453318\n",
            "Trained Weights: w1=-0.46648928204033197, w2=-0.4359901951175754, bias=-0.4970707984738248\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-0.46648928204033197\t-0.4359901951175754\t-0.4970707984738248\t0.40540867868408265\n",
            "-1\t1\t-0.46648928204033197\t-0.4359901951175754\t-0.4970707984738248\t-0.46657171155106825\n",
            "1\t-1\t-0.46648928204033197\t-0.4359901951175754\t-0.4970707984738248\t-0.5275698853965813\n",
            "1\t1\t-0.46648928204033197\t-0.4359901951175754\t-0.4970707984738248\t-1.3995502756317322\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for NOR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NOR(-1, -1) -> 1\n",
        "    (-1,  1, -1),  # NOR(-1,  1) -> -1\n",
        "    ( 1, -1, -1),  # NOR( 1, -1) -> -1\n",
        "    ( 1,  1, -1)   # NOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\tY\n",
            "-1\t-1\t1\t-0.1\t-0.1\t0.1\t0.0\n",
            "-1\t1\t1\t-0.19\t-0.009999999999999995\t0.19\t0.1\n",
            "1\t-1\t1\t-0.091\t-0.109\t0.28900000000000003\t0.010000000000000009\n",
            "1\t1\t-1\t-0.1999\t-0.21789999999999998\t0.18010000000000004\t0.08900000000000002\n",
            "-1\t-1\t1\t-0.24011\t-0.25811\t0.22031000000000003\t0.5979\n",
            "-1\t1\t1\t-0.319879\t-0.178341\t0.30007900000000004\t0.20231000000000002\n",
            "1\t-1\t1\t-0.23573310000000003\t-0.26248689999999997\t0.38422490000000004\t0.15854100000000002\n",
            "1\t1\t-1\t-0.32433359000000006\t-0.35108739\t0.29562441\t-0.11399509999999996\n",
            "-1\t-1\t1\t-0.32722905100000005\t-0.353982851\t0.298519871\t0.97104539\n",
            "-1\t1\t1\t-0.40005244390000005\t-0.2811594581\t0.3713432639\t0.27176607100000005\n",
            "1\t-1\t1\t-0.32529747171000006\t-0.35591443029\t0.44609823609\t0.2524502780999999\n",
            "1\t1\t-1\t-0.40178610511900004\t-0.43240306369899995\t0.36960960268099996\t-0.23511366591000005\n",
            "-1\t-1\t1\t-0.38140622796910006\t-0.4120231865491\t0.3492297255311\t1.203798771499\n",
            "-1\t1\t1\t-0.4495449512739901\t-0.34388446324420996\t0.41736844883599\t0.3186127669511001\n",
            "1\t-1\t1\t-0.38071574735461106\t-0.412713667163589\t0.486197652755369\t0.3117079608062099\n",
            "1\t1\t-1\t-0.44999257117832797\t-0.4819904909873059\t0.4169208289316521\t-0.307231761762831\n",
            "-1\t-1\t1\t-0.4151021820685994\t-0.4471001018775773\t0.38203043982192353\t1.348903891097286\n",
            "-1\t1\t1\t-0.48009893006730486\t-0.38210335387887184\t0.447027187820629\t0.3500325200129456\n",
            "1\t-1\t1\t-0.41500209123052445\t-0.44720019271565226\t0.5121240266574094\t0.349031611632196\n",
            "1\t1\t-1\t-0.4799942655016477\t-0.5121923669867755\t0.44713185238628617\t-0.3500782572887673\n",
            "-1\t-1\t1\t-0.43606241701417675\t-0.46826051849930456\t0.4032000038988152\t1.4393184848747094\n",
            "-1\t1\t1\t-0.498962226772808\t-0.4053607087406733\t0.46609981365744646\t0.3710019024136874\n",
            "1\t-1\t1\t-0.43621205633533916\t-0.46811087917814215\t0.5288499840949152\t0.3724982956253118\n",
            "1\t1\t-1\t-0.4986647611934826\t-0.5305635840362856\t0.4663972792367718\t-0.37547295141856607\n",
            "-1\t-1\t1\t-0.44910219874682855\t-0.48100102158963154\t0.4168347167901178\t1.49562562446654\n",
            "-1\t1\t1\t-0.5106086093520971\t-0.419494610984363\t0.4783411273953863\t0.3849358939473148\n",
            "1\t-1\t1\t-0.4493313222548623\t-0.4807718980815978\t0.539618414492621\t0.3872271290276523\n",
            "1\t1\t-1\t-0.5102828416704783\t-0.5417234174972139\t0.47866689507700494\t-0.39048480584383904\n",
            "-1\t-1\t1\t-0.4572155262460086\t-0.4886561020727441\t0.4255995796525352\t1.5306731542446972\n",
            "-1\t1\t1\t-0.5177996258634286\t-0.42807200245532406\t0.48618367926995526\t0.39415900382579966\n",
            "1\t-1\t1\t-0.4574452314496137\t-0.488426396869139\t0.5465380736837702\t0.3964560558618507\n",
            "1\t1\t-1\t-0.5175118759861155\t-0.5484930414056408\t0.48647142914726843\t-0.39933355463498255\n",
            "-1\t-1\t1\t-0.462264241332213\t-0.49324540675173834\t0.43122379449336595\t1.5524763465390248\n",
            "-1\t1\t1\t-0.5222399784248289\t-0.4332696696591224\t0.4911995315859819\t0.4002426290738406\n",
            "1\t-1\t1\t-0.4624629007068565\t-0.49304674737709486\t0.5509766093039543\t0.40222922282027535\n",
            "1\t1\t-1\t-0.5220095968288567\t-0.5525934434990951\t0.491429913181954\t-0.404533038779997\n",
            "-1\t-1\t1\t-0.46540630147786616\t-0.49599014814810455\t0.43482661783096344\t1.5660329535099058\n",
            "-1\t1\t1\t-0.5249820243617936\t-0.43641442526417706\t0.4944023407148909\t0.40424277116072505\n",
            "1\t-1\t1\t-0.4655654985235211\t-0.4958309511024496\t0.5538188665531635\t0.40583474161727434\n",
            "1\t1\t-1\t-0.5248077402162403\t-0.5550731927951689\t0.4945766248604442\t-0.4075775830728072\n",
            "Trained Weights: w1=-0.5248077402162403, w2=-0.5550731927951689, bias=0.4945766248604442\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-0.5248077402162403\t-0.5550731927951689\t0.4945766248604442\t1.5744575578718534\n",
            "-1\t1\t-0.5248077402162403\t-0.5550731927951689\t0.4945766248604442\t0.46431117228151564\n",
            "1\t-1\t-0.5248077402162403\t-0.5550731927951689\t0.4945766248604442\t0.5248420774393727\n",
            "1\t1\t-0.5248077402162403\t-0.5550731927951689\t0.4945766248604442\t-0.585304308150965\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for NAND Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # NAND(-1, -1) -> 1\n",
        "    (-1,  1,  1),  # NAND(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # NAND( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # NAND( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\tY\n",
            "-1\t-1\t1\t-0.1\t-0.1\t0.1\t0.0\n",
            "-1\t1\t-1\t0.010000000000000009\t-0.21000000000000002\t-0.010000000000000009\t0.1\n",
            "1\t-1\t-1\t-0.11099999999999999\t-0.08900000000000002\t-0.131\t0.21000000000000002\n",
            "1\t1\t1\t0.02210000000000001\t0.04409999999999997\t0.0020999999999999908\t-0.331\n",
            "-1\t-1\t1\t-0.08431\t-0.06231000000000003\t0.10851\t-0.06409999999999999\n",
            "-1\t1\t-1\t0.028741000000000003\t-0.17536100000000004\t-0.004541000000000003\t0.13050999999999996\n",
            "1\t-1\t-1\t-0.09121510000000001\t-0.055404900000000035\t-0.12449710000000001\t0.19956100000000004\n",
            "1\t1\t1\t0.03589661000000001\t0.07170680999999998\t0.0026146100000000033\t-0.27111710000000006\n",
            "-1\t-1\t1\t-0.074602271\t-0.038792071000000025\t0.11311349100000001\t-0.10498880999999999\n",
            "-1\t1\t-1\t0.040290098100000005\t-0.15368444010000004\t-0.001778878099999992\t0.14892369099999997\n",
            "1\t-1\t-1\t-0.07892946791000001\t-0.03446487409000003\t-0.12099844411\t0.19219566010000005\n",
            "1\t1\t1\t0.04450981070099999\t0.08897440452099997\t0.0024408345009999943\t-0.23439278611000003\n",
            "-1\t-1\t1\t-0.06859452737110003\t-0.024129933551100047\t0.11554517257310001\t-0.13104338072099997\n",
            "-1\t1\t-1\t0.04740644926820997\t-0.14013091019041005\t-0.00045580406620998604\t0.1600097663931\n",
            "1\t-1\t-1\t-0.07130170627103105\t-0.021422754651169024\t-0.11916395960545101\t0.18708155539241003\n",
            "1\t1\t1\t0.04988713578173405\t0.09976608740159608\t0.002024882447314094\t-0.21188842052765108\n",
            "-1\t-1\t1\t-0.06487569829186755\t-0.014996746672005526\t0.1167877165209157\t-0.14762834073601605\n",
            "-1\t1\t-1\t0.051790968522210226\t-0.1316634134860833\t0.0001210497068379196\t0.16666666814077774\n",
            "1\t-1\t-1\t-0.06656657464930292\t-0.013305870314570145\t-0.11823649346467523\t0.1835754317151314\n",
            "1\t1\t1\t0.05324431919355192\t0.1065050235282847\t0.0015744003781796162\t-0.1981089384285483\n",
            "-1\t-1\t1\t-0.06257317504081378\t-0.009312470706081002\t0.11739189461254532\t-0.15817494234365698\n",
            "-1\t1\t-1\t0.05449208485391403\t-0.1263777306008088\t0.0003266347178175061\t0.17065259894727808\n",
            "1\t-1\t-1\t-0.06362756016334002\t-0.008258085583554758\t-0.11779301029943655\t0.18119645017254038\n",
            "1\t1\t1\t0.055340305441293106\t0.11070978002107837\t0.0011748553051965788\t-0.18967865604633133\n",
            "-1\t-1\t1\t-0.0611472175744244\t-0.005777742994639137\t0.11766237832091408\t-0.16487523015717492\n",
            "-1\t1\t-1\t0.056155967715645544\t-0.12308092828470908\t0.00035919303084414045\t0.17303185290069933\n",
            "1\t-1\t-1\t-0.061803641187474334\t-0.005121319381589204\t-0.11760041587227574\t0.17959608903119878\n",
            "1\t1\t1\t0.0566488964566596\t0.11333121826254473\t0.0008521217718581997\t-0.18452537644133926\n",
            "-1\t-1\t1\t-0.06026390283807502\t-0.00358158103218989\t0.11776492106659282\t-0.16912799294734612\n",
            "-1\t1\t-1\t0.057180821449172775\t-0.12102630531943769\t0.00032019677934502777\t0.17444724287247795\n",
            "1\t-1\t-1\t-0.06067191090562277\t-0.0031735729646421434\t-0.11753253557545051\t0.17852732354795547\n",
            "1\t1\t1\t0.057465891038948766\t0.11496422897992939\t0.0006052663691210186\t-0.1813780194457154\n",
            "-1\t-1\t1\t-0.05971659432602695\t-0.00221825638504633\t0.11778775173409674\t-0.17182485364975714\n",
            "-1\t1\t-1\t0.057812014641480794\t-0.11974686535255408\t0.0002591427665889906\t0.17528608967507736\n",
            "1\t-1\t-1\t-0.05996978763458159\t-0.001965063076491691\t-0.1175226595094734\t0.17781802276062386\n",
            "1\t1\t1\t0.05797596338747309\t0.11598068794556299\t0.0004230915125812834\t-0.17945751022054668\n",
            "-1\t-1\t1\t-0.059377392594572406\t-0.001372668036482505\t0.11777644749462678\t-0.17353355982045476\n",
            "-1\t1\t-1\t0.058200724610699264\t-0.11895078524175418\t0.00019833028935510588\t0.17578117205271668\n",
            "1\t-1\t-1\t-0.0595342594034816\t-0.0012158012275733099\t-0.11753665372482576\t0.17734984014180855\n",
            "1\t1\t1\t0.05829441203210646\t0.11661287020801475\t0.00029201771076230176\t-0.17828671435588067\n",
            "Trained Weights: w1=0.05829441203210646, w2=0.11661287020801475, bias=0.00029201771076230176\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t0.05829441203210646\t0.11661287020801475\t0.00029201771076230176\t-0.17461526452935888\n",
            "-1\t1\t0.05829441203210646\t0.11661287020801475\t0.00029201771076230176\t0.05861047588667059\n",
            "1\t-1\t0.05829441203210646\t0.11661287020801475\t0.00029201771076230176\t-0.05802644046514599\n",
            "1\t1\t0.05829441203210646\t0.11661287020801475\t0.00029201771076230176\t0.1751992999508835\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for XOR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  1),  # XOR(-1, -1) -> 1\n",
        "    (-1,  1,  -1),  # XOR(-1,  1) -> -1\n",
        "    ( 1, -1,  -1),  # XOR( 1, -1) -> -1\n",
        "    ( 1,  1, 1)   # XOR( 1,  1) -> 1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\ty\tW1\tW2\tb\tY\n",
            "-1\t-1\t-1\t0.1\t0.1\t-0.1\t0.0\n",
            "-1\t1\t1\t-0.010000000000000009\t0.21000000000000002\t0.010000000000000009\t-0.1\n",
            "1\t-1\t1\t0.11099999999999999\t0.08900000000000002\t0.131\t-0.21000000000000002\n",
            "1\t1\t-1\t-0.02210000000000001\t-0.04409999999999997\t-0.0020999999999999908\t0.331\n",
            "-1\t-1\t-1\t0.08431\t0.06231000000000003\t-0.10851\t0.06409999999999999\n",
            "-1\t1\t1\t-0.028741000000000003\t0.17536100000000004\t0.004541000000000003\t-0.13050999999999996\n",
            "1\t-1\t1\t0.09121510000000001\t0.055404900000000035\t0.12449710000000001\t-0.19956100000000004\n",
            "1\t1\t-1\t-0.03589661000000001\t-0.07170680999999998\t-0.0026146100000000033\t0.27111710000000006\n",
            "-1\t-1\t-1\t0.074602271\t0.038792071000000025\t-0.11311349100000001\t0.10498880999999999\n",
            "-1\t1\t1\t-0.040290098100000005\t0.15368444010000004\t0.001778878099999992\t-0.14892369099999997\n",
            "1\t-1\t1\t0.07892946791000001\t0.03446487409000003\t0.12099844411\t-0.19219566010000005\n",
            "1\t1\t-1\t-0.04450981070099999\t-0.08897440452099997\t-0.0024408345009999943\t0.23439278611000003\n",
            "-1\t-1\t-1\t0.06859452737110003\t0.024129933551100047\t-0.11554517257310001\t0.13104338072099997\n",
            "-1\t1\t1\t-0.04740644926820997\t0.14013091019041005\t0.00045580406620998604\t-0.1600097663931\n",
            "1\t-1\t1\t0.07130170627103105\t0.021422754651169024\t0.11916395960545101\t-0.18708155539241003\n",
            "1\t1\t-1\t-0.04988713578173405\t-0.09976608740159608\t-0.002024882447314094\t0.21188842052765108\n",
            "-1\t-1\t-1\t0.06487569829186755\t0.014996746672005526\t-0.1167877165209157\t0.14762834073601605\n",
            "-1\t1\t1\t-0.051790968522210226\t0.1316634134860833\t-0.0001210497068379196\t-0.16666666814077774\n",
            "1\t-1\t1\t0.06656657464930292\t0.013305870314570145\t0.11823649346467523\t-0.1835754317151314\n",
            "1\t1\t-1\t-0.05324431919355192\t-0.1065050235282847\t-0.0015744003781796162\t0.1981089384285483\n",
            "-1\t-1\t-1\t0.06257317504081378\t0.009312470706081002\t-0.11739189461254532\t0.15817494234365698\n",
            "-1\t1\t1\t-0.05449208485391403\t0.1263777306008088\t-0.0003266347178175061\t-0.17065259894727808\n",
            "1\t-1\t1\t0.06362756016334002\t0.008258085583554758\t0.11779301029943655\t-0.18119645017254038\n",
            "1\t1\t-1\t-0.055340305441293106\t-0.11070978002107837\t-0.0011748553051965788\t0.18967865604633133\n",
            "-1\t-1\t-1\t0.0611472175744244\t0.005777742994639137\t-0.11766237832091408\t0.16487523015717492\n",
            "-1\t1\t1\t-0.056155967715645544\t0.12308092828470908\t-0.00035919303084414045\t-0.17303185290069933\n",
            "1\t-1\t1\t0.061803641187474334\t0.005121319381589204\t0.11760041587227574\t-0.17959608903119878\n",
            "1\t1\t-1\t-0.0566488964566596\t-0.11333121826254473\t-0.0008521217718581997\t0.18452537644133926\n",
            "-1\t-1\t-1\t0.06026390283807502\t0.00358158103218989\t-0.11776492106659282\t0.16912799294734612\n",
            "-1\t1\t1\t-0.057180821449172775\t0.12102630531943769\t-0.00032019677934502777\t-0.17444724287247795\n",
            "1\t-1\t1\t0.06067191090562277\t0.0031735729646421434\t0.11753253557545051\t-0.17852732354795547\n",
            "1\t1\t-1\t-0.057465891038948766\t-0.11496422897992939\t-0.0006052663691210186\t0.1813780194457154\n",
            "-1\t-1\t-1\t0.05971659432602695\t0.00221825638504633\t-0.11778775173409674\t0.17182485364975714\n",
            "-1\t1\t1\t-0.057812014641480794\t0.11974686535255408\t-0.0002591427665889906\t-0.17528608967507736\n",
            "1\t-1\t1\t0.05996978763458159\t0.001965063076491691\t0.1175226595094734\t-0.17781802276062386\n",
            "1\t1\t-1\t-0.05797596338747309\t-0.11598068794556299\t-0.0004230915125812834\t0.17945751022054668\n",
            "-1\t-1\t-1\t0.059377392594572406\t0.001372668036482505\t-0.11777644749462678\t0.17353355982045476\n",
            "-1\t1\t1\t-0.058200724610699264\t0.11895078524175418\t-0.00019833028935510588\t-0.17578117205271668\n",
            "1\t-1\t1\t0.0595342594034816\t0.0012158012275733099\t0.11753665372482576\t-0.17734984014180855\n",
            "1\t1\t-1\t-0.05829441203210646\t-0.11661287020801475\t-0.00029201771076230176\t0.17828671435588067\n",
            "Trained Weights: w1=-0.05829441203210646, w2=-0.11661287020801475, bias=-0.00029201771076230176\n",
            "X1\tX2\tW1\tW2\tb\tY\n",
            "-1\t-1\t-0.05829441203210646\t-0.11661287020801475\t-0.00029201771076230176\t0.17461526452935888\n",
            "-1\t1\t-0.05829441203210646\t-0.11661287020801475\t-0.00029201771076230176\t-0.05861047588667059\n",
            "1\t-1\t-0.05829441203210646\t-0.11661287020801475\t-0.00029201771076230176\t0.05802644046514599\n",
            "1\t1\t-0.05829441203210646\t-0.11661287020801475\t-0.00029201771076230176\t-0.1751992999508835\n"
          ]
        }
      ],
      "source": [
        "# Adaline Learning for XNOR Gate\n",
        "\n",
        "# Training Data (Inputs and Expected Outputs)\n",
        "data = [\n",
        "    (-1, -1,  -1),  # XNOR(-1, -1) -> -1\n",
        "    (-1,  1,  1),  # XNOR(-1,  1) -> 1\n",
        "    ( 1, -1,  1),  # XNOR( 1, -1) -> 1\n",
        "    ( 1,  1, -1)   # XNOR( 1,  1) -> -1\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = [0.0, 0.0]  # Weight vector for two inputs\n",
        "bias = 0.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training Phase (Adaline)\n",
        "print(f\"X1\\tX2\\ty\\tW1\\tW2\\tb\\tY\")\n",
        "for epoch in range(10):  # Run for 10 epochs\n",
        "    for x1, x2, y in data:\n",
        "        # Calculate net input (linear output before activation)\n",
        "        net = w[0] * x1 + w[1] * x2 + bias\n",
        "\n",
        "        # Calculate the predicted output\n",
        "        predicted = net  # Since Adaline doesn't apply a step function\n",
        "\n",
        "        # Calculate the error (difference between target and predicted)\n",
        "        error = y - predicted\n",
        "\n",
        "        # Update weights and bias using the Adaline learning rule\n",
        "        w[0] += learning_rate * error * x1\n",
        "        w[1] += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        # Print weights, bias, and error during training\n",
        "        print(f\"{x1}\\t{x2}\\t{y}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n",
        "\n",
        "# Final trained weights and bias\n",
        "print(f\"Trained Weights: w1={w[0]}, w2={w[1]}, bias={bias}\")\n",
        "\n",
        "# Testing Phase (Adaline)\n",
        "print(f\"X1\\tX2\\tW1\\tW2\\tb\\tY\")\n",
        "for x1, x2, y in data:\n",
        "    net = w[0] * x1 + w[1] * x2 + bias\n",
        "    predicted = net  # Adaline output\n",
        "    print(f\"{x1}\\t{x2}\\t{w[0]}\\t{w[1]}\\t{bias}\\t{predicted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
